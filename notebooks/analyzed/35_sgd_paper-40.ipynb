{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:55:48.186291Z",
     "start_time": "2022-03-19T20:55:48.177068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.0\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from math import comb\n",
    "from functools import reduce\n",
    "\n",
    "import bokeh\n",
    "import hiplot as hip\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_bokeh\n",
    "import ray\n",
    "from bson.objectid import ObjectId\n",
    "from matplotlib import pyplot as plt\n",
    "from omegaconf import OmegaConf\n",
    "from pymongo import MongoClient\n",
    "from math import comb\n",
    "from functools import reduce\n",
    "import seaborn as sns\n",
    "\n",
    "import xplogger\n",
    "\n",
    "print(xplogger.__version__)\n",
    "from xplogger.experiment_manager.record.record_list import RecordList\n",
    "from xplogger.experiment_manager.store.mongo import MongoStore\n",
    "from xplogger.experiment_manager.viz import bokeh as bokeh_viz\n",
    "from xplogger.experiment_manager.slurm import job as slurm_job_utils\n",
    "from xplogger.experiment_manager.notebook import utils as notebook_utils\n",
    "from xplogger.parser.experiment import (\n",
    "    Experiment,\n",
    "    ExperimentSequence,\n",
    "    ExperimentSequenceDict,\n",
    ")\n",
    "from xplogger.parser.experiment import utils as experiment_utils\n",
    "\n",
    "sys.path.append(\"/private/home/sodhani/projects/abstraction_by_gating/\")\n",
    "\n",
    "from math import sqrt\n",
    "from time import time\n",
    "\n",
    "from IPython.display import JSON\n",
    "from notebooks import utils, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:55:49.223740Z",
     "start_time": "2022-03-19T20:55:49.203423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1003\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1003\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\": \"dM3QQsP+wXdHg42wTqW85BjZQdLNNIXqlPw/BgKoExPmTG7ZLML4EGqLMfqHT6ON\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\": \"8x57I4YuIfu8XyZfFo0XVr2WAT8EK4rh/uDe3wF7YuW2FNUSNEpJbsPaB1nJ2fz2\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\": \"3QTqdz9LyAm2i0sG5XTePsHec3UHWwVsrOL68SYRoAXsafvfAyqtQ+h440+qIBhS\"};\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1003\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1003\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\": \"dM3QQsP+wXdHg42wTqW85BjZQdLNNIXqlPw/BgKoExPmTG7ZLML4EGqLMfqHT6ON\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\": \"8x57I4YuIfu8XyZfFo0XVr2WAT8EK4rh/uDe3wF7YuW2FNUSNEpJbsPaB1nJ2fz2\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\": \"3QTqdz9LyAm2i0sG5XTePsHec3UHWwVsrOL68SYRoAXsafvfAyqtQ+h440+qIBhS\"};\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1003\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option(\"plotting.backend\", \"pandas_bokeh\")\n",
    "pandas_bokeh.output_notebook()\n",
    "\n",
    "%matplotlib inline\n",
    "sns.color_palette(\"deep\")\n",
    "\n",
    "plt.rc('axes', titlesize=18)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=13)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=13)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=13)    # legend fontsize\n",
    "plt.rc('font', size=13)          # controls default text sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:55:50.095320Z",
     "start_time": "2022-03-19T20:55:49.894376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   priority    job_id partition job_step_name     user    state  \\\n",
      "0   1010141  58191047    devlab          bash  sodhani  RUNNING   \n",
      "\n",
      "         start_time        time  time_limit  num_nodes tres_per_node  \\\n",
      "0  2022-06-04T14:21  1-01:16:54  2-12:00:00          1         gpu:1   \n",
      "\n",
      "   min_cpus min_memory_size       nodelist mongo_id project git_issue_id  \\\n",
      "0        10            120G  learnfair0385                                 \n",
      "\n",
      "  script_id  \n",
      "0            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0605 15:38:25.219990311 2248357 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "slurm_status = slurm_job_utils.get_running_jobs_as_df()\n",
    "print(slurm_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:55:51.066753Z",
     "start_time": "2022-03-19T20:55:51.064035Z"
    }
   },
   "outputs": [],
   "source": [
    "# slurm_status[\"TIME\"] > \"2-13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:55:51.890836Z",
     "start_time": "2022-03-19T20:55:51.885478Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-05 15:38:25,886\tINFO worker.py:862 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"abstraction_by_gating\"\n",
    "\n",
    "use_record_ids = False\n",
    "ray.init(num_cpus=20, ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:55:52.622181Z",
     "start_time": "2022-03-19T20:55:52.536998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num documents: 43900\n"
     ]
    }
   ],
   "source": [
    "mongo_store = MongoStore(\n",
    "    config={\n",
    "        \"host\": \"localhost\",\n",
    "        \"port\": 27017,\n",
    "        \"db\": \"project\",\n",
    "        \"collection_name\": collection_name,\n",
    "    }\n",
    ")\n",
    "print(f\"Num documents: {mongo_store.collection.count_documents({})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:55:58.867262Z",
     "start_time": "2022-03-19T20:55:52.886597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.898404359817505\n"
     ]
    }
   ],
   "source": [
    "# start_time = time()\n",
    "# mongo_records = mongo_store.ray_get_records()\n",
    "# print(time() - start_time)\n",
    "start_time = time()\n",
    "mongo_records = mongo_store.get_records(query={})\n",
    "print(time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:05.341674Z",
     "start_time": "2022-03-19T20:55:58.869309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.3002960681915283\n"
     ]
    }
   ],
   "source": [
    "mongo_to_record = {}\n",
    "issue_to_mongo_record = {}\n",
    "unanalyzed_records = []\n",
    "start = time()\n",
    "for record in mongo_records:\n",
    "    mongo_record_id = record.id\n",
    "    mongo_to_record[mongo_record_id] = record\n",
    "    issue_id = str(record[\"setup\"][\"git\"][\"issue_id\"])\n",
    "    if issue_id not in issue_to_mongo_record:\n",
    "        issue_to_mongo_record[issue_id] = []\n",
    "    issue_to_mongo_record[issue_id].append(record)\n",
    "    if record[\"status\"] != \"ANALYZED\":\n",
    "        unanalyzed_records.append(record)\n",
    "\n",
    "print(f\"Time taken: {time() - start}\")\n",
    "assert len(unanalyzed_records) == len(mongo_store.get_unanalyzed_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:05.493063Z",
     "start_time": "2022-03-19T20:56:05.343578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unanalayzed records: 32055\n",
      "Counter({'1': 15942, '3': 4809, '25': 2751, '4': 2304, '36': 1408, '28': 1152, '29': 1123, '22': 962, '3-1': 480, '121': 372, 'dummy': 220, '35': 194, '31': 168, '18': 158, '30': 12})\n",
      "Time taken: 0.058550357818603516\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "record_list = RecordList(unanalyzed_records[:])\n",
    "print(f\"Total number of unanalayzed records: {len(record_list)}\")\n",
    "unique_issues = record_list.get_unique_issues()\n",
    "print(unique_issues)\n",
    "print(f\"Time taken: {time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n",
      "35\n",
      "{'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}\n"
     ]
    }
   ],
   "source": [
    "# record_list.mark_analyzed(collection=mongo_store.collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:10.507474Z",
     "start_time": "2022-03-19T20:56:05.495178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614\n",
      "Number of selected records: 420\n",
      "Number of uniques runs: 420\n",
      "Number of unanalyzed runs: 0\n",
      "Set of issues: {35}\n",
      "Set of script ids: {35, 29}\n"
     ]
    }
   ],
   "source": [
    "record_list = []\n",
    "for issue_id in [\"35\"]:\n",
    "    # for issue_id in issue_to_mongo_record:\n",
    "    record_list += issue_to_mongo_record[issue_id]\n",
    "\n",
    "\n",
    "def key_func(record):\n",
    "    return record[\"setup\"][\"id\"]\n",
    "\n",
    "def filter_fn(record):\n",
    "    return True\n",
    "#     return record[\"dataloader\"][\"task_specific_cfgs\"][\"living_or_not\"][\"train_config\"][\"model\"][\"should_use_pretrained\"]\n",
    "#     return record[\"setup\"][\"script_id\"] not in [\"3-3\", \"3-4\"]\n",
    "\n",
    "print(len(record_list))\n",
    "record_list = RecordList([x for x in record_list if filter_fn(x)])\n",
    "# RecordList([x for x in record_list.ray_make_oc_records() if x[\"status\"]!=\"ANALYZED\" and \"id\" in x]).mark_analyzed(collection=mongo_store.collection)\n",
    "slurm_id_to_record_map = record_list.map_to_slurm_id()\n",
    "record_list = record_list.get_unique(key_func).ray_make_oc_records()\n",
    "# print(f\"Time taken: {time() - start}\")\n",
    "print(f\"Number of selected records: {len(record_list)}\")\n",
    "print(\n",
    "    f\"Number of uniques runs: {len(set(record['setup']['id'] for record in record_list))}\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of unanalyzed runs: {len(set(record['setup']['id'] for record in record_list if record['status']!='ANALYZED'))}\"\n",
    ")\n",
    "print(\n",
    "    f\"Set of issues: {set(record['setup']['git']['issue_id'] for record in record_list)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Set of script ids: {set(record['setup']['script_id'] for record in record_list)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:23.886909Z",
     "start_time": "2022-03-19T20:56:23.884481Z"
    }
   },
   "outputs": [],
   "source": [
    "# record_list = RecordList(list(x for x in record_list if x.setup.script_id == 25))\n",
    "\n",
    "# record_list = RecordList(list(x for x in record_list if x.setup.script_id == \"25-1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:24.831403Z",
     "start_time": "2022-03-19T20:56:24.828556Z"
    }
   },
   "outputs": [],
   "source": [
    "# for r in record_list:\n",
    "#     model_cfg = r.model\n",
    "#     if (\n",
    "#         model_cfg[\"hidden_layer_cfg\"][\"dim\"] == 128\n",
    "#         and model_cfg[\"num_layers\"] == 1\n",
    "#         and model_cfg[\"weight_init\"][\"bias\"] == 0\n",
    "#         and model_cfg[\"weight_init\"][\"gain\"] == 1.0\n",
    "#         and model_cfg[\"gate_cfg\"][\"mode\"] == \"4_plus_mod\"\n",
    "#         #             and exp.config[\"experiment\"][\"task_one\"][\"transform\"] == \"default\"\n",
    "#         #             and exp.config[\"experiment\"][\"task_two\"][\"transform\"] == \"invert\"\n",
    "#     ):\n",
    "#         print(r)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:25.339763Z",
     "start_time": "2022-03-19T20:56:25.335661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(record_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:25.900499Z",
     "start_time": "2022-03-19T20:56:25.897706Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_record_list = []\n",
    "# for r in record_list:\n",
    "#     if \"optimizer._target_.optimizer.lr\" in r[\"setup\"][\"viz\"][\"params\"]:\n",
    "#         new_record_list.append(r)\n",
    "# new_record_list = RecordList(new_record_list)\n",
    "\n",
    "# record_list.delete(collection=mongo_store.collection, delete_from_filesystem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:27.069855Z",
     "start_time": "2022-03-19T20:56:26.507549Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0605 15:30:58.405495815 2248357 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/sodhani/projects/abstraction_by_gating/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:27.075198Z",
     "start_time": "2022-03-19T20:56:27.072336Z"
    }
   },
   "outputs": [],
   "source": [
    "# record_list.delete(collection=mongo_store.collection, delete_from_filesystem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:29.613683Z",
     "start_time": "2022-03-19T20:56:27.619195Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2248357/3943563090.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mhyperparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_experiment_sequence_dict_groups_and_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mviz_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_experiment_from_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;32m~/projects/xplogger/xplogger/experiment_manager/record/record_list.py\u001b[0m in \u001b[0;36mmake_experiment_sequence_dict_groups_and_hyperparams\u001b[0;34m(self, viz_params, load_experiment_from_dir)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_groups_and_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviz_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mviz_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         experiment_sequence_dict = ExperimentSequenceDict(\n\u001b[0;32m--> 249\u001b[0;31m             {\n\u001b[0m\u001b[1;32m    250\u001b[0m                 key: record_list.load_experiments(\n\u001b[1;32m    251\u001b[0m                     \u001b[0mload_experiment_from_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_experiment_from_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/xplogger/xplogger/experiment_manager/record/record_list.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    248\u001b[0m         experiment_sequence_dict = ExperimentSequenceDict(\n\u001b[1;32m    249\u001b[0m             {\n\u001b[0;32m--> 250\u001b[0;31m                 key: record_list.load_experiments(\n\u001b[0m\u001b[1;32m    251\u001b[0m                     \u001b[0mload_experiment_from_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_experiment_from_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 )\n",
      "\u001b[0;32m~/projects/xplogger/xplogger/experiment_manager/record/record_list.py\u001b[0m in \u001b[0;36mload_experiments\u001b[0;34m(self, load_experiment_from_dir)\u001b[0m\n\u001b[1;32m    227\u001b[0m     ) -> ExperimentSequence:\n\u001b[1;32m    228\u001b[0m         \u001b[0;34m\"\"\"Load experiments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         experiments = [\n\u001b[0m\u001b[1;32m    230\u001b[0m             base_record.load_experiment(\n\u001b[1;32m    231\u001b[0m                 \u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/xplogger/xplogger/experiment_manager/record/record_list.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;34m\"\"\"Load experiments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         experiments = [\n\u001b[0;32m--> 230\u001b[0;31m             base_record.load_experiment(\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mload_experiment_from_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_experiment_from_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/xplogger/xplogger/experiment_manager/record/base.py\u001b[0m in \u001b[0;36mload_experiment\u001b[0;34m(record, load_experiment_from_dir)\u001b[0m\n\u001b[1;32m     35\u001b[0m ) -> Experiment:\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m\"\"\"Load experiment given a record.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     return load_experiment_from_dir(  # type: ignore\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logbook\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logger_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     )\n",
      "\u001b[0;32m/tmp/ipykernel_2248357/3943563090.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(log_dir)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m load_experiment_from_dir = lambda log_dir: utils.load_experiment_from_dir(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcache_failed_experiment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mskip_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/abstraction_by_gating/notebooks/utils.py\u001b[0m in \u001b[0;36mload_experiment_from_dir\u001b[0;34m(log_dir, cache_failed_experiment, verbose, skip_cache, force_cache)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mexperiment_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDLNParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     exp = experiment_parser.parse(\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmetric_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_parser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/abstraction_by_gating/notebooks/parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, filepath_pattern, metric_parser, cache_failed_experiment, skip_cache, verbose, force_cache)\u001b[0m\n\u001b[1;32m     64\u001b[0m                             \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minfo_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                         \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minfo_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{paths[0].parent}/metric_log.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         metrics = parse_metric_file(\n\u001b[1;32m     68\u001b[0m             \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "load_experiment_from_dir = lambda log_dir: utils.load_experiment_from_dir(\n",
    "    log_dir=log_dir,\n",
    "    cache_failed_experiment=True,\n",
    "    verbose=False,\n",
    "    skip_cache=False,\n",
    ")\n",
    "\n",
    "viz_params = record_list.get_viz_params()\n",
    "\n",
    "viz_params.remove(\"experiment\")\n",
    "start_time = time()\n",
    "# (\n",
    "#     exp_seq_dict,\n",
    "#     groups,\n",
    "#     hyperparams,\n",
    "# ) = record_list.ray_make_experiment_sequence_dict_groups_and_hyperparams(\n",
    "#     viz_params, load_experiment_from_dir\n",
    "# )\n",
    "\n",
    "(\n",
    "    exp_seq_dict,\n",
    "    groups,\n",
    "    hyperparams,\n",
    ") = record_list.make_experiment_sequence_dict_groups_and_hyperparams(\n",
    "    viz_params, load_experiment_from_dir\n",
    ")\n",
    "\n",
    "print(f\"Time taken: {time() - start_time}\")\n",
    "\n",
    "\n",
    "notebook_utils.prettyprint_dict(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:34.103831Z",
     "start_time": "2022-03-19T20:56:34.099216Z"
    }
   },
   "outputs": [],
   "source": [
    "viz_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:34.743295Z",
     "start_time": "2022-03-19T20:56:34.732518Z"
    }
   },
   "outputs": [],
   "source": [
    "print(list(exp_seq_dict.values())[0])\n",
    "print(type(list(exp_seq_dict.values())[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:35.070839Z",
     "start_time": "2022-03-19T20:56:35.059988Z"
    }
   },
   "outputs": [],
   "source": [
    "exp = list(exp_seq_dict.values())[0][0]\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "exp.metrics[\"train_epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:29.619907Z",
     "start_time": "2022-03-19T20:56:29.619890Z"
    }
   },
   "outputs": [],
   "source": [
    "list(exp_seq_dict.values())[0][0].config[\"setup\"][\"viz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-19T20:56:29.817387Z",
     "start_time": "2022-03-19T20:56:29.806721Z"
    }
   },
   "outputs": [],
   "source": [
    "notebook_utils.prettyprint_dict(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T00:06:50.648799Z",
     "start_time": "2022-03-15T00:06:50.648782Z"
    }
   },
   "outputs": [],
   "source": [
    "type(list(exp_seq_dict.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-15T00:06:50.650670Z",
     "start_time": "2022-03-15T00:06:50.650651Z"
    }
   },
   "outputs": [],
   "source": [
    "# y_metric_list = [\"accuracy_odd_even\", \"accuracy_greater_than_four\"]\n",
    "y_metric_list = [\"average_accuracy_for_unselected_paths\"]\n",
    "# y_metric_list = [\"accuracy\"]\n",
    "x_metric = \"epoch\"\n",
    "mode = \"test_epoch\"\n",
    "\n",
    "\n",
    "def filter_fn(key, experiment_sequence):\n",
    "    def filter_experiment(exp):\n",
    "        #         model_cfg = exp.config[\"model\"]\n",
    "        #         return (\n",
    "        #             exp is not None\n",
    "        #             and exp.metrics is not None\n",
    "        #             and \"test_epoch\" in exp.metrics\n",
    "        #             and not (\n",
    "        #                 np.isnan(\n",
    "        #                     exp.metrics[\"test_epoch\"][\n",
    "        #                         \"average_loss_for_unselected_paths\"\n",
    "        #                     ].to_numpy()[-1]\n",
    "        #                 )\n",
    "        #             )\n",
    "        #             and exp.metrics[\"test_epoch\"][\n",
    "        #                 \"average_loss_for_unselected_paths\"\n",
    "        #             ].to_numpy()[-1]\n",
    "        #             > 0.99\n",
    "        #             and\n",
    "        #             exp.config[\"model\"][\"hidden_layer_cfg\"][\"dim\"] == 128\n",
    "        #         )\n",
    "        if not exp:\n",
    "            return False\n",
    "        model_cfg = exp.config[\"model\"]\n",
    "        return (\n",
    "            exp.config[\"experiment\"][\"task\"][\"mode\"] == \"rotate_input_permute_target\"\n",
    "            and model_cfg[\"hidden_layer_cfg\"][\"dim\"] == 128\n",
    "            and model_cfg[\"num_layers\"] == 1\n",
    "            and model_cfg[\"weight_init\"][\"bias\"] == 0\n",
    "            and model_cfg[\"weight_init\"][\"gain\"] not in [0.0001, 0.001, 0.01]\n",
    "            and exp.config[\"optimizer\"][\"_target_\"] == \"torch.optim.SGD\"\n",
    "            #             and model_cfg[\"weight_init\"][\"gain\"] == 0.001\n",
    "            #             and model_cfg[\"gate_cfg\"][\"mode\"] in [\"4_plus_mod_permute\", \"4_plus_mod\"]\n",
    "            and model_cfg[\"gate_cfg\"][\"mode\"]\n",
    "            in [\"10_plus_mod_permute\"]\n",
    "#             in [\"4_plus_mod\", \"4_plus_mod_permute\", \"6_plus_mod\", \"6_plus_mod_permute\"]\n",
    "            and exp.config[\"experiment\"][\"task\"][\"num_classes_in_selected_dataset\"] == 10\n",
    "        )\n",
    "\n",
    "    return all(filter_experiment(exp) for exp in experiment_sequence)\n",
    "\n",
    "\n",
    "def get_function_to_get_experiment_name(param_list: list[DictConfig]):\n",
    "    unique_params_and_values = {}\n",
    "    words_to_ignore = []\n",
    "    for param in param_list:\n",
    "        for key, value in param.items():\n",
    "            if key not in unique_params_and_values:\n",
    "                unique_params_and_values[key] = set()\n",
    "            unique_params_and_values[key].add(value)\n",
    "    for key, value in unique_params_and_values.items():\n",
    "        if len(value) == 1:\n",
    "            words_to_ignore.append(key)\n",
    "\n",
    "    def get_experiment_name(params, **kwargs):\n",
    "        return \"-\".join(\n",
    "            [\n",
    "                f\"{val}\"\n",
    "                for key, val in sorted(\n",
    "                    list((OmegaConf.to_container(params)).items())\n",
    "                    + list(kwargs.items())\n",
    "                )\n",
    "                if all(word not in key for word in words_to_ignore)\n",
    "            ]\n",
    "        )\n",
    "        return \"-\".join(\n",
    "            [\n",
    "                f\"{val}\"\n",
    "                for key, val in sorted(\n",
    "                    list((OmegaConf.to_container(params)).items())\n",
    "                    + list(kwargs.items())\n",
    "                )\n",
    "                if all(word not in key for word in words_to_ignore)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return get_experiment_name\n",
    "\n",
    "\n",
    "# def get_experiment_name(params, **kwargs):\n",
    "#     words_to_ignore = [\"issue_id\", \"env\"]\n",
    "#     return params[\"model.gate_cfg.mode\"]\n",
    "#     return \"-\".join(\n",
    "#         [\n",
    "#             f\"{val}\"\n",
    "#             for key, val in sorted(\n",
    "#                 list((OmegaConf.to_container(params)).items()) + list(kwargs.items())\n",
    "#             )\n",
    "#             if all(word not in key for word in words_to_ignore)\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "\n",
    "def plot(\n",
    "    filter_fn,\n",
    "    y_metric_list: list[str] = [\"return_mean\"],\n",
    "    x_metric: str = \"frames\",\n",
    "    mode: str = \"train\",\n",
    "    metadata_for_plot={},\n",
    "    p=None,\n",
    "    color_palette=None,\n",
    "    color_offset: int = 0,\n",
    "    colors=None,\n",
    "):\n",
    "\n",
    "    filtered_exp_seq_dict = ExperimentSequenceDict(\n",
    "        {\n",
    "            key: ExperimentSequence(experiment_sequence)\n",
    "            for key, experiment_sequence in sorted(\n",
    "                exp_seq_dict.items(),\n",
    "                key=lambda item: sorted(OmegaConf.to_container(item[0]).items()),\n",
    "            )\n",
    "            if filter_fn(key, experiment_sequence)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fn_to_get_experiment_name = get_function_to_get_experiment_name(\n",
    "        param_list=list(filtered_exp_seq_dict.keys())\n",
    "    )\n",
    "\n",
    "    color_palette = bokeh.palettes.d3[\"Category20\"]\n",
    "\n",
    "    return bokeh_viz.plot_experiment_sequence_dict(\n",
    "        exp_seq_dict=filtered_exp_seq_dict,\n",
    "        kwargs_for_aggregate_metrics = {\n",
    "            \"get_experiment_name\": fn_to_get_experiment_name,\n",
    "            \"x_name\": x_metric,\n",
    "            \"mode\": mode,\n",
    "            \"drop_duplicates\": True,\n",
    "            \"dropna\": True,\n",
    "            \"verbose\": True,\n",
    "            \"metric_names\": y_metric_list,\n",
    "            \"x_min\": 100,\n",
    "            \"x_max\": 1000,\n",
    "        },\n",
    "        metadata_for_plot=metadata_for_plot,\n",
    "        color_palette=color_palette,\n",
    "        p=p,\n",
    "        colors=colors,\n",
    "        color_offset=color_offset,\n",
    "    )\n",
    "        \n",
    "\n",
    "title = \"title\"\n",
    "color_palette = bokeh.palettes.d3[\"Category20\"]\n",
    "colors = color_palette[20][::1]\n",
    "\n",
    "p = None\n",
    "\n",
    "p = plot(\n",
    "    filter_fn=filter_fn,\n",
    "    metadata_for_plot={\"title\": title, \"fill_alpha\": 0.2},\n",
    "    y_metric_list=y_metric_list,\n",
    "    x_metric=x_metric,\n",
    "    color_palette=color_palette,\n",
    "    mode=mode,\n",
    "    p=p,\n",
    "    color_offset=0,\n",
    "    colors=colors,\n",
    ")\n",
    "\n",
    "p.legend.click_policy = \"hide\"\n",
    "# p.legend.visible=\n",
    "\n",
    "\n",
    "bokeh.plotting.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:48:31.338528Z",
     "start_time": "2022-01-27T13:48:30.743180Z"
    }
   },
   "outputs": [],
   "source": [
    "# param_list = list(filtered_exp_seq_dict.keys())\n",
    "\n",
    "x = np.asarray([0.1, 1.0, 10.0])\n",
    "\n",
    "K = 10\n",
    "#permute input\n",
    "data_y = {\n",
    "    10: {\n",
    "        \"trained\": np.asarray([91.4, 92.4, 88.1]),\n",
    "        \"untrained\": np.asarray([91.3, 65, 37.6]),\n",
    "    }\n",
    "}\n",
    "\n",
    "# rotate input\n",
    "# data_y = {\n",
    "#     10: {\n",
    "#         \"trained\": np.asarray([91.2, 92.4, 88.1]),\n",
    "#         \"untrained\": np.asarray([84.1, 70.3, 38.8]),\n",
    "#     }\n",
    "# }\n",
    "\n",
    "for mode in [\"trained\", \"untrained\"]:\n",
    "    plt.plot(x, (100 - data_y[K][mode]) / 100, label = f\"{mode} pathways\", marker = \"x\")\n",
    "    \n",
    "plt.ylabel(\"error rate\")\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"gain ratio (initialization scale)\")\n",
    "\n",
    "plt.title(f\"MNIST-Permuted-Input-Permuted-Output-40, K={K}\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(\n",
    "    f\"/private/home/sodhani/projects/abstraction_by_gating/results/mnist-permute-input-40-gain-ratio-{K}-percent-128.pdf\",\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:29:27.301793Z",
     "start_time": "2022-01-27T13:29:27.298615Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_combination_to_hyperparams(combination):\n",
    "    combination_dict = {}\n",
    "    for key, value in zip(hyperparams.keys(), combination):\n",
    "        combination_dict[key] = value\n",
    "    return combination_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:29:27.474222Z",
     "start_time": "2022-01-27T13:29:27.470342Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_metadata(\n",
    "    mode: str,\n",
    "    max_epochs: int,\n",
    "    min_epochs: int,\n",
    "    metric_name: str,\n",
    "):\n",
    "    metadata = OmegaConf.create(\n",
    "        {\n",
    "            \"metric_name\": metric_name,\n",
    "            \"mode\": mode,\n",
    "            \"x\": {\"name\": \"epoch\", \"min\": min_epochs, \"max\": max_epochs},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    step_metadata = OmegaConf.create(\n",
    "        {\n",
    "            \"metric_name\": \"epoch\",\n",
    "            \"mode\": mode,\n",
    "            \"x\": {\"name\": \"epoch\", \"min\": min_epochs, \"max\": max_epochs},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return metadata, step_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:29:27.640814Z",
     "start_time": "2022-01-27T13:29:27.638436Z"
    }
   },
   "outputs": [],
   "source": [
    "# list(exp_seq_dict.values())[0].aggregate_metrics(\n",
    "#         metric_names=[metadata.metric_name],\n",
    "#         x_name=metadata.x.name,\n",
    "#         x_min=metadata.x.min,\n",
    "#         x_max=metadata.x.max,\n",
    "#         mode=metadata.mode,\n",
    "#         drop_duplicates=True,\n",
    "#         verbose=True,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:29:27.816233Z",
     "start_time": "2022-01-27T13:29:27.805664Z"
    }
   },
   "outputs": [],
   "source": [
    "type(list(exp_seq_dict.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:29:38.347620Z",
     "start_time": "2022-01-27T13:29:27.953135Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "mode = \"test_epoch\"\n",
    "# metric_name = \"accuracy_task_two_encoder_task_one_decoder\"\n",
    "\n",
    "metrics = [\n",
    "    \"average_accuracy_for_selected_paths\",\n",
    "    \"average_accuracy_for_unselected_paths\",\n",
    "    \"average_loss_for_selected_paths\",\n",
    "    \"average_loss_for_unselected_paths\",\n",
    "]\n",
    "\n",
    "df_dict = {}\n",
    "\n",
    "for metric_name in metrics:\n",
    "    metadata, step_metadata = make_metadata(\n",
    "        mode=mode,\n",
    "        max_epochs=1000,\n",
    "        min_epochs=1,\n",
    "        metric_name=metric_name,\n",
    "    )\n",
    "\n",
    "    df_dict[metric_name] = notebook_utils.make_df(\n",
    "        metadata=metadata,\n",
    "        step_metadata=step_metadata,\n",
    "        groups=groups,\n",
    "        hyperparams=hyperparams,\n",
    "        exp_seq_dict=exp_seq_dict,\n",
    "    )\n",
    "    \n",
    "#     df_dict[key].drop(\n",
    "#         [],\n",
    "#         axis=1,\n",
    "#         inplace=True,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:29:38.426090Z",
     "start_time": "2022-01-27T13:29:38.349464Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dict[\"average_accuracy_for_unselected_paths\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:29:38.530650Z",
     "start_time": "2022-01-27T13:29:38.427433Z"
    }
   },
   "outputs": [],
   "source": [
    "keys_to_merge_on = [\n",
    "    \"model.hidden_layer_cfg.dim\",\n",
    "    \"experiment.task.num_classes_in_selected_dataset\",\n",
    "    \"experiment.num_epochs\",\n",
    "    \"dataloader.name\",\n",
    "    \"model.decoder_cfg.should_share\",\n",
    "    \"model.weight_init.bias\",\n",
    "    \"experiment.task.mode\",\n",
    "    \"model.num_layers\",\n",
    "    \"optimizer._target_\",\n",
    "    \"setup.script_id\",\n",
    "    \"model.weight_init.gain\",\n",
    "    \"model.weight_init.should_do\",\n",
    "    \"model.encoder_cfg.should_share\",\n",
    "    \"experiment.task.num_input_transformations\",\n",
    "    \"optimizer.lr\",\n",
    "    \"optimizer.momentum\",\n",
    "    \"model.should_use_non_linearity\",\n",
    "    \"model.gate_cfg.mode\",\n",
    "    \"dataloader.train_config.dataloader.batch_size\",\n",
    "    \"model.pretrained_cfg.should_use\",\n",
    "    \"model.hidden_layer_cfg.should_share\",\n",
    "    \"steps\",\n",
    "    \"seeds\",\n",
    "#     \"experiment.name\",\n",
    "    \n",
    "    \n",
    "#     \"model.encoder_cfg.should_share\",\n",
    "# #     \"experiment.name\",\n",
    "#     \"model.weight_init.should_do\",\n",
    "#     \"model.weight_init.gain\",\n",
    "#     \"model.weight_init.bias\",\n",
    "#     \"optimizer._target_\",\n",
    "#     \"optimizer.lr\",\n",
    "#     \"model.hidden_layer_cfg.dim\",\n",
    "#     \"model.should_use_non_linearity\",\n",
    "#     \"model.hidden_layer_cfg.should_share\",\n",
    "#     \"model.num_layers\",\n",
    "#     \n",
    "#     \"experiment.task.num_classes_in_selected_dataset\",\n",
    "#     \"model.gate_cfg.mode\",\n",
    "#     \"experiment.num_epochs\",\n",
    "#     \"dataloader.train_config.dataloader.batch_size\",\n",
    "#     \"optimizer.momentum\"\n",
    "]\n",
    "merged_df = df_dict[metrics[0]]\n",
    "for current_metric in metrics[1:]:\n",
    "    merged_df = merged_df.merge(\n",
    "        df_dict[current_metric], on=keys_to_merge_on, suffixes=[None, None]\n",
    "    )\n",
    "\n",
    "merged_df = merged_df.drop(\n",
    "    [\n",
    "        \"seeds\",\n",
    "        \"steps\",\n",
    "#         \"experiment.name\",\n",
    "#         \"experiment.task.num_classes_in_original_dataset\",\n",
    "#         \"model.gate_cfg.mode\",\n",
    "        \"experiment.num_epochs\",\n",
    "        \"mean_average_loss_for_unselected_paths\",\n",
    "        \"mean_average_loss_for_selected_paths\"\n",
    "    ]\n",
    "    + [f\"stderr_{metric_name}\" for metric_name in metrics],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "merged_df.style.set_sticky(axis=\"columns\")\n",
    "\n",
    "merged_df\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# cm = sns.light_palette(\"red\", as_cmap=True)\n",
    "\n",
    "# merged_df.style.background_gradient(\n",
    "#     axis=1, subset=[f\"mean_return_mean_{key}\" for key in keys], vmax=1.4, vmin=0.6\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:29:38.572000Z",
     "start_time": "2022-01-27T13:29:38.532348Z"
    }
   },
   "outputs": [],
   "source": [
    "df_to_plot = merged_df\n",
    "\n",
    "# keys_to_drop = [\"model.should_share_hidden_layer\", \"model.name\", \"model.should_share_decoder\", \n",
    "#                 \"stderr_accuracy_greater_than_four_encoder_odd_even_decoder\", \n",
    "#                 \"steps\", \"seeds\", \n",
    "#                 \"model.num_layers\", \"model.weight_init.should_do\", \n",
    "#                 \"model.should_share_encoder\", \"model.hidden_size\"\n",
    "#                ]\n",
    "# df_to_plot = df_to_plot.drop(keys_to_drop, axis=1)\n",
    "\n",
    "hip.Experiment.from_dataframe(df_to_plot).display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:29:38.575467Z",
     "start_time": "2022-01-27T13:29:38.573305Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_to_plot.to_feather(\"/private/home/sodhani/projects/abstraction_by_gating/notebooks/feather/25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:29:38.594663Z",
     "start_time": "2022-01-27T13:29:38.576730Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_classes = 10\n",
    "# dim = 128\n",
    "# lr = 0.0001\n",
    "# df = df_to_plot[df_to_plot[\"experiment.task.num_classes_in_selected_dataset\"] == num_classes]\n",
    "# # df = df[(df[\"optimizer.lr\"] == lr) & (df[\"model.hidden_layer_cfg.dim\"] == dim)]\n",
    "# df = df.groupby(\"model.gate_cfg.mode\").max()\n",
    "# # x = df[\"model.gate_cfg.mode\"]\n",
    "# # y = df[\"mean_average_accuracy_for_selected_paths\"]\n",
    "# # print(x, y)\n",
    "# df = df.reset_index()\n",
    "# df.plot.bar(x=\"model.gate_cfg.mode\", y=\"mean_average_accuracy_for_unselected_paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:29:38.615116Z",
     "start_time": "2022-01-27T13:29:38.596170Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_classes_in_selected_dataset = 10\n",
    "# dim = 128\n",
    "# optimizer_lr = 0.0001\n",
    "# # experiment_task_mode = \"rotate_input_permute_target\"\n",
    "# experiment_task_mode = \"permute_input_permute_target\"\n",
    "# num_input_transformations = 100\n",
    "\n",
    "\n",
    "# def get_filters(df):\n",
    "#     filters = {\n",
    "#         \"experiment.task.num_classes_in_selected_dataset\": num_classes_in_selected_dataset,\n",
    "#         \"experiment.task.mode\": experiment_task_mode,\n",
    "#         \"optimizer.lr\": optimizer_lr,\n",
    "#         \"model.hidden_layer_cfg.dim\": dim,\n",
    "#         \"experiment.task.num_input_transformations\": num_input_transformations,\n",
    "#     }\n",
    "#     return reduce(\n",
    "#         lambda x, y: x & y, [df[key] == val for key, val in filters.items()], True\n",
    "#     )\n",
    "\n",
    "\n",
    "# df = df_to_plot[get_filters(df_to_plot)]\n",
    "\n",
    "# x_metric_name = \"model.gate_cfg.mode\"\n",
    "# # y_metric_name = \"mean_average_accuracy_for_selected_paths\"\n",
    "\n",
    "# for y_metric_name in [\"mean_average_accuracy_for_selected_paths\", \"mean_average_accuracy_for_unselected_paths\"]:\n",
    "\n",
    "#     points = []\n",
    "#     for x, y in zip(df[x_metric_name], df[y_metric_name]):\n",
    "#         x = ds.make_transformed_x(x)\n",
    "#         points.append(ds.Point(x, y))\n",
    "\n",
    "#     points.sort()\n",
    "\n",
    "#     x_y = {}\n",
    "#     # x, y = [], []\n",
    "#     for p in points:\n",
    "#         x = float(p.x.normalize(num_input_transformations=num_input_transformations))\n",
    "#         if x not in x_y:\n",
    "#             x_y[x] = y\n",
    "#         else:\n",
    "#             x_y[x] = max(x_y[x], p.y)\n",
    "\n",
    "#     print(x_y)\n",
    "#     # plt.plot(x, y)\n",
    "#     if y_metric_name == \"mean_average_accuracy_for_unselected_paths\":\n",
    "#         label = \"untrained pathways\"\n",
    "#     else:\n",
    "#         label = \"trained pathways\"\n",
    "#     plt.plot(x_y.keys(), x_y.values(), label=label, marker = \"x\")\n",
    "#     # print(x)\n",
    "#     # print(y)\n",
    "\n",
    "# if num_input_transformations == 100:\n",
    "#     plt.title(r'$10^{4}$ datasets')\n",
    "    \n",
    "# plt.xlabel(\"% of datasets used for training\")\n",
    "# # plt.ylabel(\" \".join(y_metric_name.split(\"_\")[:3]))\n",
    "# plt.ylabel(\"mean accuracy\")\n",
    "\n",
    "# lower_range = 0.25\n",
    "# upper_range = 0.95\n",
    "\n",
    "# plt.yticks(np.arange(lower_range, upper_range, 0.1))\n",
    "# plt.xticks(np.arange(10, 100, 10))\n",
    "# plt.ylim([lower_range, upper_range])\n",
    "\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # plt.savefig(\n",
    "# # #     f\"/private/home/sodhani/projects/abstraction_by_gating/results/mnist-{experiment_task_mode.replace('_', '-')}-{num_input_transformations}/{y_metric_name}_{dim}.pdf\",\n",
    "# #     f\"/private/home/sodhani/projects/abstraction_by_gating/results/mnist-{experiment_task_mode.replace('_', '-')}-{num_input_transformations}/line_{dim}.pdf\",\n",
    "# #     dpi=300,\n",
    "# #     bbox_inches=\"tight\",\n",
    "# # )\n",
    "# # plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T13:29:56.235023Z",
     "start_time": "2022-01-27T13:29:55.859223Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes_in_selected_dataset = 10\n",
    "dim = 128\n",
    "optimizer_lr = 0.0001\n",
    "# experiment_task_mode = \"rotate_input_permute_target\"\n",
    "experiment_task_mode = \"permute_input_permute_target\"\n",
    "num_input_transformations = 100\n",
    "\n",
    "\n",
    "def get_filters(df):\n",
    "    filters = {\n",
    "        \"experiment.task.num_classes_in_selected_dataset\": num_classes_in_selected_dataset,\n",
    "        \"experiment.task.mode\": experiment_task_mode,\n",
    "        \"optimizer.lr\": optimizer_lr,\n",
    "        \"model.hidden_layer_cfg.dim\": dim,\n",
    "        \"experiment.task.num_input_transformations\": num_input_transformations,\n",
    "    }\n",
    "    return reduce(\n",
    "        lambda x, y: x & y, [df[key] == val for key, val in filters.items()], True\n",
    "    )\n",
    "\n",
    "\n",
    "df = df_to_plot[get_filters(df_to_plot)]\n",
    "\n",
    "x_metric_name = \"model.gate_cfg.mode\"\n",
    "# y_metric_name = \"mean_average_accuracy_for_selected_paths\"\n",
    "\n",
    "for y_metric_name in [\"mean_average_accuracy_for_selected_paths\", \"mean_average_accuracy_for_unselected_paths\"]:\n",
    "\n",
    "    points = []\n",
    "    for x, y in zip(df[x_metric_name], df[y_metric_name]):\n",
    "        x = ds.make_transformed_x(x)\n",
    "        points.append(ds.Point(x, y))\n",
    "\n",
    "    points.sort()\n",
    "\n",
    "    x_y = {}\n",
    "    # x, y = [], []\n",
    "    for p in points:\n",
    "        x = float(p.x.normalize(num_input_transformations=num_input_transformations))\n",
    "        if x not in x_y:\n",
    "            x_y[x] = y\n",
    "        else:\n",
    "            x_y[x] = max(x_y[x], p.y)\n",
    "\n",
    "    print(x_y)\n",
    "    # plt.plot(x, y)\n",
    "    if y_metric_name == \"mean_average_accuracy_for_unselected_paths\":\n",
    "        label = \"untrained pathways\"\n",
    "    else:\n",
    "        label = \"trained pathways\"\n",
    "    plt.plot(x_y.keys(), x_y.values(), label=label, marker = \"x\")\n",
    "    # print(x)\n",
    "    # print(y)\n",
    "\n",
    "if num_input_transformations == 100:\n",
    "    plt.title(r'MNIST-Permuted-Input-Permuted-Output-100')\n",
    "elif experiment_task_mode == \"rotate_input_permute_target\":\n",
    "    plt.title(r'MNIST-Rotated-Input-Permuted-Output-40')\n",
    "elif experiment_task_mode == \"permute_input_permute_target\":\n",
    "    plt.title(r'MNIST-Permuted-Input-Permuted-Output-40')\n",
    "    \n",
    "plt.xlabel(\"% of datasets used for training\")\n",
    "# plt.ylabel(\" \".join(y_metric_name.split(\"_\")[:3]))\n",
    "plt.ylabel(\"mean accuracy\")\n",
    "\n",
    "lower_range = 0.25\n",
    "upper_range = 0.95\n",
    "\n",
    "plt.yticks(np.arange(lower_range, upper_range, 0.1))\n",
    "plt.xticks(np.arange(10, 100, 10))\n",
    "plt.ylim([lower_range, upper_range])\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "path = f\"/private/home/sodhani/projects/abstraction_by_gating/results/mnist-{experiment_task_mode.replace('_', '-')}-{num_input_transformations}/line_{dim}.pdf\"\n",
    "# print(path)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(\n",
    "#     f\"/private/home/sodhani/projects/abstraction_by_gating/results/mnist-{experiment_task_mode.replace('_', '-')}-{num_input_transformations}/{y_metric_name}_{dim}.pdf\",\n",
    "    path,\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T22:58:59.297395Z",
     "start_time": "2022-01-26T22:58:59.288769Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from typing import Any, Optional\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from xplogger.parser.experiment import ExperimentSequenceDict  # type: ignore\n",
    "\n",
    "\n",
    "# y_metric_list = [\"accuracy_odd_even\", \"accuracy_greater_than_four\"]\n",
    "y_metric_list = [\"average_accuracy_for_selected_paths\"]\n",
    "# y_metric_list = [\"accuracy\"]\n",
    "x_metric = \"epoch\"\n",
    "mode = \"test_epoch\"\n",
    "\n",
    "\n",
    "def filter_fn(key, experiment_sequence):\n",
    "    def filter_experiment(exp):\n",
    "        if not exp:\n",
    "            return False\n",
    "        model_cfg = exp.config[\"model\"]\n",
    "        return (\n",
    "            exp.config[\"experiment\"][\"task\"][\"mode\"] == \"permute_input_permute_target\"\n",
    "            and model_cfg[\"hidden_layer_cfg\"][\"dim\"] == 128\n",
    "            and model_cfg[\"num_layers\"] == 1\n",
    "            and model_cfg[\"weight_init\"][\"bias\"] == 0\n",
    "#             and model_cfg[\"weight_init\"][\"gain\"] not in [0.0001, 0.001, 0.01]\n",
    "            and exp.config[\"optimizer\"][\"_target_\"] == \"torch.optim.SGD\"\n",
    "            #             and model_cfg[\"weight_init\"][\"gain\"] == 0.001\n",
    "            #             and model_cfg[\"gate_cfg\"][\"mode\"] in [\"4_plus_mod_permute\", \"4_plus_mod\"]\n",
    "            and model_cfg[\"gate_cfg\"][\"mode\"]\n",
    "            in [\"90_plus_mod_permute\"]\n",
    "#             in [\"4_plus_mod\", \"4_plus_mod_permute\", \"6_plus_mod\", \"6_plus_mod_permute\"]\n",
    "            and exp.config[\"experiment\"][\"task\"][\"num_classes_in_selected_dataset\"] == 10\n",
    "        )\n",
    "\n",
    "    return all(filter_experiment(exp) for exp in experiment_sequence)\n",
    "\n",
    "\n",
    "def get_function_to_get_experiment_name(param_list: list[DictConfig]):\n",
    "    unique_params_and_values = {}\n",
    "    words_to_ignore = []\n",
    "    for param in param_list:\n",
    "        for key, value in param.items():\n",
    "            if key not in unique_params_and_values:\n",
    "                unique_params_and_values[key] = set()\n",
    "            unique_params_and_values[key].add(value)\n",
    "    for key, value in unique_params_and_values.items():\n",
    "        if len(value) == 1:\n",
    "            words_to_ignore.append(key)\n",
    "\n",
    "    def get_experiment_name(params, **kwargs):\n",
    "        return params[\"model.weight_init.gain\"]\n",
    "    return get_experiment_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T22:59:04.313172Z",
     "start_time": "2022-01-26T22:59:03.958003Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def matplotlib_plot_experiment_sequence_dict(\n",
    "    exp_seq_dict: ExperimentSequenceDict,\n",
    "    metadata_for_plot: dict[str, Any],\n",
    "    color_palette: list[Any],\n",
    "    plt: Optional[figure],\n",
    "    colors: Optional[list[str]] = None,\n",
    "    color_offset: int = 0,\n",
    "    return_all_metrics_with_same_length: bool = True,\n",
    "    kwargs_for_aggregate_metrics: Optional[dict[str, Any]] = None,\n",
    ") -> figure:\n",
    "    \"\"\"Plot the given experiment sequence dict as a matplotlib.\n",
    "\n",
    "    Args:\n",
    "        exp_seq_dict (ExperimentSequenceDict):\n",
    "        metadata_for_plot (dict[str, Any]):\n",
    "        color_palette (list[Any]):\n",
    "        p (Optional[figure]):\n",
    "        colors (Optional[list[str]], optional): Defaults to None.\n",
    "        color_offset (int, optional): Defaults to 0.\n",
    "        kwargs_for_aggregate_metrics (Optional[dict[str, Any]], optional):\n",
    "            These arguments are pass to aggregation function of exp_seq_dict.\n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        figure:\n",
    "    \"\"\"\n",
    "    if not kwargs_for_aggregate_metrics:\n",
    "        kwargs_for_aggregate_metrics = {}\n",
    "\n",
    "    for key in [\n",
    "        \"get_experiment_name\",\n",
    "        \"metric_names\",\n",
    "        \"x_name\",\n",
    "        \"x_min\",\n",
    "        \"x_max\",\n",
    "        \"mode\",\n",
    "        \"drop_duplicates\",\n",
    "        \"dropna\",\n",
    "        \"verbose\",\n",
    "    ]:\n",
    "        assert key in kwargs_for_aggregate_metrics\n",
    "\n",
    "    x_metric = kwargs_for_aggregate_metrics[\"x_name\"]\n",
    "    y_metric_list = kwargs_for_aggregate_metrics[\"metric_names\"]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    data = exp_seq_dict.aggregate_metrics(\n",
    "        return_all_metrics_with_same_length=return_all_metrics_with_same_length,\n",
    "        **kwargs_for_aggregate_metrics,\n",
    "    )\n",
    "\n",
    "    if colors is None:\n",
    "        try:\n",
    "            colors = color_palette[len(data) + color_offset]\n",
    "        except KeyError:\n",
    "            # this could be because we have fewer data points than 3\n",
    "            colors = color_palette[3][: len(data) + color_offset]\n",
    "    assert colors is not None\n",
    "    for index, (key, y) in enumerate(data.items(), color_offset):\n",
    "        if key.endswith(f\"_{x_metric}\"):\n",
    "            continue\n",
    "        for current_metric_name in y_metric_list:\n",
    "            if key.endswith(current_metric_name):\n",
    "                current_exp_seq_key = key.replace(f\"_{current_metric_name}\", \"\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"Can not find the metric name.\")\n",
    "            breakpoint()\n",
    "        x_key = f\"{current_exp_seq_key}_{x_metric}\"\n",
    "        x = data[x_key].mean(axis=0)[::20]\n",
    "        mean = y.mean(axis=0)[::20]\n",
    "        stderr = y.std(axis=0) / math.sqrt(len(y))\n",
    "        gain, mode = key.split(\"_\", 1)\n",
    "        if mode == \"average_accuracy_for_unselected_paths\":\n",
    "            marker = \"x\"\n",
    "            label = \"untrained\"\n",
    "        else:\n",
    "            marker = \".\"\n",
    "            label = \"trained\"\n",
    "        label = f\"{label}, gain={gain}\"\n",
    "        print(key, mode)\n",
    "        plt.plot(x, mean, linewidth=2, color=colors[index], label=label, marker=marker)\n",
    "        # p.varea(\n",
    "        #     x,\n",
    "        #     mean - stderr,\n",
    "        #     mean + stderr,\n",
    "        #     fill_alpha=metadata_for_plot.get(\"fill_alpha\", 0.6),\n",
    "        #     color=colors[index],\n",
    "        # )\n",
    "\n",
    "    # p.legend.location = \"bottom_right\"\n",
    "    # p.legend.click_policy = \"hide\"\n",
    "    # return p\n",
    "\n",
    "    \n",
    "def plot(\n",
    "    filter_fn,\n",
    "    y_metric_list: list[str] = [\"return_mean\"],\n",
    "    x_metric: str = \"frames\",\n",
    "    mode: str = \"train\",\n",
    "    metadata_for_plot={},\n",
    "    p=None,\n",
    "    color_palette=None,\n",
    "    color_offset: int = 0,\n",
    "    colors=None,\n",
    "):\n",
    "\n",
    "    filtered_exp_seq_dict = ExperimentSequenceDict(\n",
    "        {\n",
    "            key: ExperimentSequence(experiment_sequence)\n",
    "            for key, experiment_sequence in sorted(\n",
    "                exp_seq_dict.items(),\n",
    "                key=lambda item: sorted(OmegaConf.to_container(item[0]).items()),\n",
    "            )\n",
    "            if filter_fn(key, experiment_sequence)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fn_to_get_experiment_name = get_function_to_get_experiment_name(\n",
    "        param_list=list(filtered_exp_seq_dict.keys())\n",
    "    )\n",
    "\n",
    "    color_palette = bokeh.palettes.d3[\"Category20\"]\n",
    "\n",
    "    return matplotlib_plot_experiment_sequence_dict(\n",
    "        exp_seq_dict=filtered_exp_seq_dict,\n",
    "        kwargs_for_aggregate_metrics = {\n",
    "            \"get_experiment_name\": fn_to_get_experiment_name,\n",
    "            \"x_name\": x_metric,\n",
    "            \"mode\": mode,\n",
    "            \"drop_duplicates\": True,\n",
    "            \"dropna\": True,\n",
    "            \"verbose\": True,\n",
    "            \"metric_names\": y_metric_list,\n",
    "            \"x_min\": 100,\n",
    "            \"x_max\": 1000,\n",
    "        },\n",
    "        metadata_for_plot=metadata_for_plot,\n",
    "        color_palette=color_palette,\n",
    "        plt = plt,\n",
    "#         p=p,\n",
    "        colors=colors,\n",
    "        color_offset=color_offset,\n",
    "    )\n",
    "        \n",
    "\n",
    "color_palette = bokeh.palettes.d3[\"Category20\"]\n",
    "colors = color_palette[20][::1]\n",
    "\n",
    "\n",
    "plot(\n",
    "    filter_fn=filter_fn,\n",
    "    metadata_for_plot={\"title\": title, \"fill_alpha\": 0.2},\n",
    "    y_metric_list=y_metric_list,\n",
    "    x_metric=x_metric,\n",
    "    color_palette=color_palette,\n",
    "    mode=mode,\n",
    "    p=p,\n",
    "    color_offset=0,\n",
    "    colors=colors,\n",
    ")\n",
    "\n",
    "plt.title(r'$10^{4}$ datasets')\n",
    "plt.ylabel(\"mean accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "\n",
    "# lower_range = 0.25\n",
    "# upper_range = 0.95\n",
    "\n",
    "# plt.yticks(np.arange(lower_range, upper_range, 0.1))\n",
    "# plt.xticks(np.arange(10, 100, 10))\n",
    "# plt.ylim([lower_range, upper_range])\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.savefig(\n",
    "# #     f\"/private/home/sodhani/projects/abstraction_by_gating/results/mnist-{experiment_task_mode.replace('_', '-')}-{num_input_transformations}/{y_metric_name}_{dim}.pdf\",\n",
    "#     f\"/private/home/sodhani/projects/abstraction_by_gating/results/mnist-{experiment_task_mode.replace('_', '-')}-{num_input_transformations}/line_{dim}.pdf\",\n",
    "#     dpi=300,\n",
    "#     bbox_inches=\"tight\",\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abstraction_by_gating",
   "language": "python",
   "name": "abstraction_by_gating"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
